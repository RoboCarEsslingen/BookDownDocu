[
["index.html", "RoboCar project documentation Chapter 1 Project RoboCar 1.1 How to join us 1.2 FAQ", " RoboCar project documentation Uwe Sterr 2020-02-24 Chapter 1 Project RoboCar We want to build autonomous driving model cars which can master a course without any human intervention The project was started during one of those cold dark winter evenings on 8th of February 2018 by Elias, who carried this idea with him for quite some time already Andreas, who as far as I can tell is interested in just about everything Finn, who probably never thought about this idea before that day but was immediately enthusiastic Uwe, who was infected with the idea by Elias in autumn 2017 We decided then and there to build autonomous robotic cars probably with Raspberry Pi, but open for all possible concepts. Later in 2018 we decided to work with the donkeycar framework. 1.1 How to join us We organize our meetings via the meetup group https://www.meetup.com/Esslingen-Makerspace/, you are welcome to join in, register as a member of that meetup group and you will receive invitations to our meetings. 1.2 FAQ Frequent questions are addressed in this document: Selection of RC car =&gt; chapter 2.1.1 Wiring of donkeycar =&gt; chapter 2.3 DonkeyCar software analysis 5 Connect donkeycar to WiFi and Bluetooth =&gt; chapter 3 Useful links =&gt; chapter 13 For inspiration you can check the https://www.hs-esslingen.de/hochschule/aktuelles/news/artikel/news/ganz-vorne-mit-dabei/ "],
["donkeyCar.html", "Chapter 2 Building and operate a donkeycar 2.1 Reason for changing the plattform 2.2 What to do 2.3 Rewiring the car 2.4 Folders created during installation of host PC 2.5 Calibrate the car 2.6 Drive your car with web interface 2.7 Connecting bluetooth controller 2.8 Start driving or training 2.9 Tips on how to drive during training 2.10 Mange recorded data 2.11 Train neural network 2.12 Run car on neural net 2.13 See Rainer’s description 2.14 Build pre processing pipe line 2.15 bluetooth key mapping 2.16 Load h5 model and inspect", " Chapter 2 Building and operate a donkeycar In December 2018 I switched from sunfounder to donkeycar Donkey Car 2.1 Reason for changing the plattform The sunfounder was wonderful to learn the basics, PWM, I2C, use of camera with Pi, but had shortcomings in the steering mechanism. The steering angle was not reproducible, i.e. sending twice the same steering command did result in different steering angles of about a few degrees difference. Now since the aim of the game is to train a neural net which takes the commanded steering angle as ground truth this was a show stopper for me and therefore i moved to =&gt; Donkey Car Second reason, other members of the group Esslinger Makerspace Projekt: Autonomen RoboCar bauen opted for donkey car. 2.1.1 Selecting a RC car Selection criteria for RC car are: Brushed motor =&gt; no need for high speed NiMH battery =&gt; less sensitive to mechanical impact compared to Li-ion battery ESC not integrated into remote control receiver 3 wire steering servo A great buying guide for RC-cars from the donkeycar project can be found at http://docs.donkeycar.com/roll_your_own/ I changed to Reely Dart 2.0 Brushed Tamiya-Buchse is the connector of the NiMH battery Charger is Voltcraft V-Charge Eco NiMh 3000 Figure 2.1: Reely Dart 2.0 Brushed converted RoboCar 2.2 What to do disassemble the sunfounder, by taking of the plate on which the raspberry is mounted disassemble the L298N H-bridge, not needed if you use an ESC Rewire the PWM signal of the car channel 1 =&gt; steering channel 0 =&gt; throttle Install Software on Pi and Host Computer Setup Raspberry Pi Setup Mac Host PC (or windows or Linux host) Calibrate your car Drive your car Train an autopilot with Keras http://docs.donkeycar.com/guide/simulator/ Select web or physical controller Controller Parts 2.3 Rewiring the car The rewiring consists of Disconnect the following connections PWM signal RC receiver =&gt; Steering Servo PWM signal RC receiver =&gt; ESC Connect Battery =&gt; DC/DC converter DC/DC converter =&gt; Raspberry Pi PWM Servo Hat onto Raspberry Pi PWM Servo Hat Ch 0 =&gt; ESC PWM Servo Hat Ch 1 =&gt; Steering Servo Raspberry Pi =&gt; Raspberry Pi camera The resulting wiring diagram can be seen in figure 2.2 Figure 2.2: Wiring diagram after rewiring car WARNING, don’t connect the red wire of the ESC to PWM Servo Hat, only the other two wires are needed between ESC to PWM Servo Hat So there are only the PWM signal wires from the PWM Servo Hat to the car and the power supply needs to be connected to the DC/DC converter. A close-up on the PWM Servo Hat is given in figure 2.3 Figure 2.3: Close-up of PWM Servo Hat You can eliminate the RC receiver altogether, see figure 2.4 Figure 2.4: Rewiring without RC receiver WARNING, don’t connect the red wire of the ESC to PWM Servo Hat, only the other two wires are needed between ESC to PWM Servo Hat 2.3.1 Parts list The following list is what I used to build the car, this does not mean there are better and/or cheaper options available. In total the cost is about 250€. Reely Dart 2.0 Brushed ca. 130€ Raspberry Pi ca. 34€ Raspbery Pi camera ca. 25€ PWM Servo Hat ca. 32€ DC/DC converter ca. 4€, WiiU controller ca. 18€ Tamiya Stecker Power wires ca. 9€ Stuff to mechanically integrate the new elements into the car. 2.3.2 Why not using PCA9685 Note, there seems to be an issue with driving two servos with PCA9685 at the same time. What happens is that steering and throttle work during calibration but not when the car is controlled via the web interface. TBC A possible cure is described at Adafruit 16 Channel Servo Driver with Raspberry Pi. When to add an optional Capacitor to the driver board We have a spot on the PCB for soldering in an electrolytic capacitor. Based on your usage, you may or may not need a capacitor. If you are driving a lot of servos from a power supply that dips a lot when the servos move, n * 100uF where n is the number of servos is a good place to start - eg 470uF or more for 5 servos. Since its so dependent on servo current draw, the torque on each motor, and what power supply, there is no “one magic capacitor value” we can suggest which is why we don’t include a capacitor in the kit. I did not check whether this works. The now used PWM SERVO HAT 2327 By ADAFRUIT INDUSTRIES did not show any problem controlling two channels at the same time 2.4 Folders created during installation of host PC /Users/uwesterr/mycar/models /Users/uwesterr/mycar/data /Users/uwesterr/mycar/logs 2.5 Calibrate the car Instructions for the calibrations can be found at http://docs.donkeycar.com/guide/calibrate/#calibrate-your-car ssh into your raspberry 2.5.1 Setting WIFI up for the raspberry To set up the WiFi connection on the raspberry you find a detailed description at SETTING WIFI UP VIA THE COMMAND LINE 2.5.2 Steering Calibration To start steering calibration run donkey calibrate --channel &lt;your_steering_channel&gt; PWM channel 1 for steering donkey calibrate --channel 1 (env) pi@donkeypi_uwe:~ $ donkey calibrate --channel 1 using donkey version: 2.5.8 ... Enter a PWM setting to test(0-1500)360 Enter a PWM setting to test(0-1500)120 . . . Ctrl C to exit calibration mode Values for my car: left:490 right: 390 stop 410 slow forward 440 slow reverse 390 fast forward 550 fast reverse 310 2.5.3 Throttle Calibration To start throttle calibration run donkey calibrate --channel &lt;your_throttle_channel&gt; PWM channel 0 for throttle (env) pi@donkeypi_uwe:~ $ donkey calibrate --channel 0 using donkey version: 2.5.8 ... Enter a PWM setting to test(0-1500)400 Enter a PWM setting to test(0-1500)420 Enter a PWM setting to test(0-1500)400 Enter a PWM setting to test(0-1500)380 2.5.4 Config.py after calibration after configuration the config.py file reads &quot;&quot;&quot; CAR CONFIG This file is read by your car application&#39;s manage.py script to change the car performance. EXMAPLE ----------- import dk cfg = dk.load_config(config_path=&#39;~/mycar/config.py&#39;) print(cfg.CAMERA_RESOLUTION) &quot;&quot;&quot; import os #PATHS CAR_PATH = PACKAGE_PATH = os.path.dirname(os.path.realpath(__file__)) DATA_PATH = os.path.join(CAR_PATH, &#39;data&#39;) MODELS_PATH = os.path.join(CAR_PATH, &#39;models&#39;) #VEHICLE DRIVE_LOOP_HZ = 20 MAX_LOOPS = 100000 #CAMERA CAMERA_RESOLUTION = (120, 160) #(height, width) CAMERA_FRAMERATE = DRIVE_LOOP_HZ #STEERING STEERING_CHANNEL = 1 STEERING_LEFT_PWM = 460 STEERING_RIGHT_PWM = 370 #THROTTLE THROTTLE_CHANNEL = 0 THROTTLE_FORWARD_PWM = 550 THROTTLE_STOPPED_PWM = 410 THROTTLE_REVERSE_PWM = 320 #TRAINING BATCH_SIZE = 128 TRAIN_TEST_SPLIT = 0.8 #JOYSTICK USE_JOYSTICK_AS_DEFAULT = False JOYSTICK_MAX_THROTTLE = 0.25 JOYSTICK_STEERING_SCALE = 1.0 AUTO_RECORD_ON_THROTTLE = True TUB_PATH = os.path.join(CAR_PATH, &#39;tub&#39;) # if using a single tub 2.6 Drive your car with web interface Instructions are at http://docs.donkeycar.com/guide/get_driving/ 2.6.1 Preparation on Mac open terminal Activate mappings to donkey Python setup with: source activate donkey Change to your local dir for managing donkey: cd ~/mycar 2.6.2 Preparation on Raspberry cd ~/mycar python manage.py drive in the terminal window that looks like follows: (env) pi@donkeypi_uwe:~/mycar $ python manage.py drive using donkey version: 2.5.8 ... /usr/lib/python3/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters loading config file: /home/pi/mycar/config.py config loaded PiCamera loaded.. .warming camera Starting Donkey Server... You can now go to http://127.0.0.1:8887 to drive your car. /home/pi/env/lib/python3.5/site-packages/picamera/encoders.py:544: PiCameraResolutionRounded: frame size rounded up from 160x120 to 160x128 width, height, fwidth, fheight))) 2.6.3 Open control in web browser on Mac You can now control your car from a web browser at the URL: 192.168.178.67:8887 2.7 Connecting bluetooth controller I got an Wii U second hand WiiU controller, therefore I did select the Controllers page the WiiU controller After ssh into the raspberry install Bluetooth Game Controller library git clone https://github.com/autorope/donkeypart_bluetooth_game_controller.git pip install -e ./donkeypart_bluetooth_game_controller 2.7.1 Connect your bluetooth controller to the raspberry pi. Start the Bluetooth bash tool on your raspberry pi. sudo bluetoothctl power on scan on Turn on your controller in scan mode and look for your controllers name in the bluetoothctl scan results. This is done by turning over the controller and pushing the sync button until the 4 blue buttons blink In the shell the scan showed &gt; [NEW] Device 0C:FC:83:97:A6:4F Nintendo RVL-CNT-01 Connect to your controller using its id (my controller id is 0C:FC:83:97:A6:4F) once you’ve found it’s id. You may have to run these commands several times. pair 0C:FC:83:97:A6:4F connect 0C:FC:83:97:A6:4F trust 0C:FC:83:97:A6:4F Now your controller should show that your controller is connected - the 4 blinking lights turns to one solid light. Run the part script to see if it works. You should see all the button values printed as you press them. Like this. cd donkeypart_bluetooth_game_controller/donkeypart_bluetooth_game_controller (env) pi@donkeypi_uwe:~/donkeypart_bluetooth_game_controller/donkeypart_bluetooth_game_controller $ python part.py log Please give a string that can identify the bluetooth device (ie. nintendo)nintendo log device /dev/input/event0, name &quot;Nintendo Wii Remote Pro Controller&quot;, phys &quot;&quot; button: LEFT_STICK_X, value:-0.009375 button: LEFT_STICK_X, value:-0.0015625 button: LEFT_STICK_X, value:-0.00390625 button: LEFT_STICK_X, value:-0.00546875 button: LEFT_STICK_X, value:-0.00703125 How to add the Bluetooth controller into manage.pyis described in section 5.1.3.3 In the file manageOrgExtWii.py the Bluetooth controller was integrated. 2.8 Start driving or training Before start driving it might be a good idea to limit the max velocity, this can be done as described in 5.5 Power up donkeycar Wait about 10s until Raspberry Pi is booted Switch on Bluetooth controller for Nintendo Wii controller the four blue LEDs flash a few times the left most LED solid shows that controller is connected with Raspberry Pi Once controller is connected proceed as described in instructions ssh pi@&lt;your_pi_ip_address&gt; # or ssh pi@donkeypi-uwe cd ~/mycar python manage.py drive # or if you have changed the file name to indicate that you added the bluetooth contorller python manageOrgExtWii.py drive how to the the IP address of your car you check your at your router. How to do this at shackspace is described 9.4 And the car drives add Bluetooth controller to manage.py https://github.com/autorope/donkey2_plus/blob/690ff1b78c49c2a3dd4c1095bfcac9673f150804/manage.py 2.8.1 Create movie from tub data To create a movie from the gathered data donkey makemovie &lt;tub_path&gt; [--out=&lt;tub_movie.mp4&gt;] # for example create movie tubMovie.mp4 from data located at tub source activate donkey donkey makemovie --tub tub --out tubMovie.mp4 More details at http://docs.donkeycar.com/utility/donkey/#make-movie-from-tub 2.8.2 Options of manage.py The script manage.py can be called with several options, to find them type (env) pi@donkeypi_uwe:~/mycar $ python manage.py drive --h using donkey version: 2.5.8 … /usr/lib/python3/dist-packages/h5py/init.py:34: FutureWarning: Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future, it will be treated as np.float64 == np.dtype(float).type. from ._conv import register_converters as _register_converters Scripts to drive a donkey 2 car and train a model for it. Usage: manage.py (drive) [--model=&lt;model&gt;] [--js] [--chaos] manage.py (train) [--tub=&lt;tub1,tub2,..tubn&gt;] (--model=&lt;model&gt;) [--base_model=&lt;base_model&gt;] [--no_cache] Options: -h --help Show this screen. --tub TUBPATHS List of paths to tubs. Comma separated. Use quotes to use wildcards. ie &quot;~/tubs/*&quot; --js Use physical joystick. --chaos Add periodic random steering when manually driving 2.9 Tips on how to drive during training Here are a few tips on how to gather the necessary training data. The tips are based on a video by Tawn Kramer Driving Tips to Train your Autonomous End-to-End NN Driver create about 10,000 images four different driving modes, more don’t hurt do some driving to get used to the control divide training session into four parts drive slowly precisely center of the lane about two laps about 10% of the data drive in lane with small oscillations shows NN how track looks like at different angles shows NN how to correct back to center about 2-3 laps drive like you normally drive speed a little bit not always bang center of lane drive with slow oscillations bounce back and forth between extremes of the lanes help NN to establish the lane boundaries The resulting steering angle histogram is shown in figure 2.5 Figure 2.5: Steering angle histogramm of Tawn Kramer’s training data About 50% are straight driving Two humps from the oscillating driving Right hump more prominent since course has more right bends 2.9.1 How to edit recorded data The donkey car utility tubclean is described in chapter 4.6 2.10 Mange recorded data The training data is being stored in the folder /mycar/tub. Since there might be a lot of data after a while you might want to delete archive copy it from the Raspberry Pi to your computer data 2.10.1 Archiving data Archiving can be done by renaming the tub folder with the mv command. The mv command moves, or renames, files and directories on your file system. To move the tub folder, i.e rename it, to tubArchive use mv tub tubArchive more details on mv can be found at https://www.computerhope.com/unix/umv.htm 2.10.2 Delete data Once you want to get rid of the data use the following command rm -rf tub More on rm can be found at https://www.computerhope.com/unix/urm.htm 2.10.3 Copy files from raspi to mac Open terminal on Mac Docy of scp command to copy file from pi to mac run in terminal scp pi@192.168.178.67:mycar/manage.py then type the password of the pi, afterwards the the file mycar/manage.py is being copied to the current location. The current location can be found via the pwdcommand. Or run rsyncin terminal on Mac rsync -r pi@192.168.178.67:~/mycar/tub/ ~/mycar/tubMac/ You will be prompted for the raspi password. All files from the raspi folder ~/mycar/tub/ will be copied to the folder ~/mycar/tubMac/on the Mac 2.10.4 How big is the tub folder If you want to know how many MB of data you collected already use the su command. More info on the su command here (env) pi@donkeypi_uwe:~/mycar $ du -sh tub 232M tub 2.11 Train neural network A description is given in the donkeycar documentation Below an example for the case that all training data are located in a single folder named tub. To train a neural network: change to directory where tub is located activate donkey environment source activate donkey run (donkey) Uwes-MacBook-Pro:mycar uwesterr$ python ~/mycar/manage.py train --tub tub --model ./models/mypilot which will place a model file called mypilot at (donkey) Uwes-MacBook-Pro:models uwesterr$ pwd /Users/uwesterr/mycar/models (donkey) Uwes-MacBook-Pro:models uwesterr$ ls mypilot the command to train the neural network leads to the following output in the terminal window (donkey) Uwes-MacBook-Pro:mycar uwesterr$ python ~/mycar/manage.py train --tub tub --model ./models/mypilot using donkey version: 2.5.7 ... loading config file: /Users/uwesterr/mycar/config.py config loaded tub_names tub train: 4497, validation: 1125 steps_per_epoch 35 Epoch 1/100 2019-04-12 16:14:59.549618: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 34/35 [============================&gt;.] - ETA: 0s - loss: 15.0006 - angle_out_loss: 12.5454 - throttle_out_loss: 17.4558 Epoch 00001: val_loss improved from inf to 0.37914, saving model to ./models/mypilot 35/35 [==============================] - 29s 815ms/step - loss: 14.5843 - angle_out_loss: 12.2074 - throttle_out_loss: 16.9612 - val_loss: 0.3791 - val_angle_out_loss: 0.5718 - val_throttle_out_loss: 0.1865 Epoch 2/100 34/35 [============================&gt;.] - ETA: 0s - loss: 0.3353 - angle_out_loss: 0.5307 - throttle_out_loss: 0.1398 Epoch 00002: val_loss improved from 0.37914 to 0.29204, saving model to ./models/mypilot 35/35 [==============================] - 31s 874ms/step - loss: 0.3338 - angle_out_loss: 0.5289 - throttle_out_loss: 0.1387 - val_loss: 0.2920 - val_angle_out_loss: 0.4629 - val_throttle_out_loss: 0.1212 2.12 Run car on neural net Run the following command on the raspberry pi. (env) pi@donkeypi_uwe:~/mycar $ python manage.py drive --js --model ~/mycar/models/vaihingenIII.h5 Toggle drive mode by pressing button ‘A’, the modes are toggles between three states Throttle and steering WiiU controlled Throttle WiiU controlled and steering controlled by neural net Throttle and steering controlled by neural net The modes have names and are defined: User : As you guessed, this is where you are in control of both the steering and throttle control. Local Angle : Not too obvious, but this is where the trained model (mypilot from above) controls the steering. The Local refers to the trained model which is locally hosted on the raspberry-pi. Local Pilot : This is where the trained model (mypilot) assumes control of both the steering and the throttle. As of now, it’s purportedly not very reliable. 2.12.1 change model change in /Users/uwesterr/CloudProjectsUnderWork/ProjectsUnderWork/RoboCar/donkeycar/donkeycar-1/donkeycar/parts/ the file keras.pyto tawn kramers keras.py which has dropout in the model. The file can be found at github https://github.com/tawnkramer/donkey/blob/master/donkeycar/parts/keras.py Model doesnt work with donkeycar installation on raspi, change to tawn kramer installation 2.13 See Rainer’s description https://github.com/tawnkramer/donkey/blob/master/docs/guide/install_software.md name of new installation donkeyUweTk (env) pi@donkeyUweTk:~/donkey/donkeycar/parts $ ls __init__.py augment.py controller.py dgym.py graph.py keras.py network.py simulation.py transform.py __pycache__ behavior.py cv.py encoder.py image.py led_status.py ros.py teensy.py web_controller actuator.py camera.py datastore.py file_watcher.py imu.py lidar.py salient.py throttle_filter.py add Wii U controller instruction https://github.com/autorope/donkeypart_bluetooth_game_controller 2.14 Build pre processing pipe line First step is to install openCV on the raspi so that the images recorded by the camera can be processed before they are stored. 2.14.1 Install openCV on the raspi To add a pre-processing pipe line we need openCV. To intsall opencv on the raspberry pip install opencv-python 2.15 bluetooth key mapping For Tawn Kramer’s version of donkeycar the following key mappings are valid for the Wii U X: erased last 100 records. B: E-Stop!!! “-” new mode: local_angle again “-” new mode: local Left cross: up: increase throttle_scale down decrease throttle_scale 2.16 Load h5 model and inspect from tensorflow.python.keras.models import load_model model1 = load_model(&quot;base_linearTawnKramer.h5&quot;) model1.summary() you get __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== img_in (InputLayer) (None, 120, 160, 3) 0 __________________________________________________________________________________________________ cropping2d (Cropping2D) (None, 120, 160, 3) 0 img_in[0][0] __________________________________________________________________________________________________ batch_normalization_v1 (BatchNo (None, 120, 160, 3) 12 cropping2d[0][0] __________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 58, 78, 24) 1824 batch_normalization_v1[0][0] __________________________________________________________________________________________________ dropout (Dropout) (None, 58, 78, 24) 0 conv2d_1[0][0] __________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 27, 37, 32) 19232 dropout[0][0] __________________________________________________________________________________________________ dropout_1 (Dropout) (None, 27, 37, 32) 0 conv2d_2[0][0] __________________________________________________________________________________________________ conv2d_3 (Conv2D) (None, 12, 17, 64) 51264 dropout_1[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 12, 17, 64) 0 conv2d_3[0][0] __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 10, 15, 64) 36928 dropout_2[0][0] __________________________________________________________________________________________________ dropout_3 (Dropout) (None, 10, 15, 64) 0 conv2d_4[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 8, 13, 64) 36928 dropout_3[0][0] __________________________________________________________________________________________________ dropout_4 (Dropout) (None, 8, 13, 64) 0 conv2d_5[0][0] __________________________________________________________________________________________________ flattened (Flatten) (None, 6656) 0 dropout_4[0][0] __________________________________________________________________________________________________ dense (Dense) (None, 100) 665700 flattened[0][0] __________________________________________________________________________________________________ dropout_5 (Dropout) (None, 100) 0 dense[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 50) 5050 dropout_5[0][0] __________________________________________________________________________________________________ dropout_6 (Dropout) (None, 50) 0 dense_1[0][0] __________________________________________________________________________________________________ n_outputs0 (Dense) (None, 1) 51 dropout_6[0][0] __________________________________________________________________________________________________ n_outputs1 (Dense) (None, 1) 51 dropout_6[0][0] ================================================================================================== Total params: 817,040 Trainable params: 817,034 Non-trainable params: 6 "],
["Conncectivity.html", "Chapter 3 Wireless connections Raspi, Controller, Mac 3.1 Purpose of connections 3.2 Setting up of the wireless connections", " Chapter 3 Wireless connections Raspi, Controller, Mac Donkeycar needs to communicate to other devices for several purposes using different connections. In this chapter the purpose and the setting up of the wireless connections of the donkeycar, i.e. Raspberry Pi are described. An overview of the necessary connection are shown in figure 3.1 Figure 3.1: Setting up wifi hotspot on Android phone (Pixel 2) 3.1 Purpose of connections The connections serve different needs which are as follows: Raspi Wifi installing and updating necessary software for ssh connection from Mac start python scripts transfer training data to Mac transfer trained model from Mac Raspi Bluetooth Connect controller Steering Throttle Switching recording of images ON/OFF Switching control user mode car is controlled by controller =&gt; user steering is controlled by neural net steering and throttle are controlled by neural net 3.2 Setting up of the wireless connections In this section the setting up of the wireless connections is described. Special focus is set on the having an independent setup where the user is not dependent on external wifi provider. This enables the user to gather trainings data on any track without the need to find a wifi connection and the assigned Raspberry Pi IP-adress 3.2.1 Bluetooth connection The bluetooth connection is described in official donkeycar documentation and for the WiiU controller in chapter 2.7.1. Quite a few people reported problems with connecting the controller when not using original WiiU or PS3 controller. 3.2.2 Wifi connection Since the wifi connection is useful to ssh into the Raspberry Pi to edit and start python scripts and move data to and from the Raspberry Pi it is helpful if the wifi connection is independent of wifi provided at a track. Therefore a setup with an Android phone (Pixel 2) is described which can be setup anywhere. 3.2.2.1 Enable wifi on Raspberry Pi At https://www.raspberrypi.org/documentation/configuration/wireless/wireless-cli.md a description on enable wifi scannig for available wifi adding network details 3.2.2.2 Setting up wifi hotspot (Android phone) Using a mobil phone as wifi hotspot enables independent operation of the donkeycar. In figure 3.2 a screen shot of the wifi hotspot menu is shown. Figure 3.2: Setting up wifi hotspot on Android phone (Pixel 2) The Raspberry Pi IP-adress can be found using a app called Network Analyzer Figure 3.3: Network Analyzer LAN scanning result Now the Raspberry Pi can be reached from the Mac using the IP-adress 192.168.43.75. In a terminal on the Mac ssh connection and file transfers can be done. #open ssh connection to Raspberry Pi ssh pi@192.168.43.75 # copy files from Raspberry Pi to Mac rsync -r pi@192.168.43.75:~/mycar/tub/ ~/mycar/tubMac/ 3.2.2.3 Get IP adress of raspi in shack If connected to a provided wifi network the Raspberry Pi IP adress needs to be found out. Check in the router for the IP adress, procedure is dependent on router At shackspace go to http://leases.shack/#/ (only available from the shackspace network) and then connect via ssh pi@ipAdress if you get Uwes-MBP:data uwesterr$ ssh pi@10.42.26.33 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:aYMRAzv3GpxqJNugz1oTi20m0QKVIVfVxszQkJNbNqg. Please contact your system administrator. Add correct host key in /Users/uwesterr/.ssh/known_hosts to get rid of this message. Offending ECDSA key in /Users/uwesterr/.ssh/known_hosts:20 ECDSA host key for 10.42.26.33 has changed and you have requested strict checking. Host key verification failed. then you have to remove the cached key for donkeypi-uwe (or the old IP adress) on the local machine: ssh-keygen -R donkeypi-uwe "],
["donkey-command-line-utilities.html", "Chapter 4 Donkey Command-line Utilities 4.1 Create Car 4.2 Find Car 4.3 Calibrate Car 4.4 Clean data in Tub 4.5 Make Movie from Tub 4.6 Check Tub 4.7 Histogram 4.8 Plot Predictions 4.9 Simulation Server 4.10 Colab training 4.11 Transfer learning", " Chapter 4 Donkey Command-line Utilities This chapter is copied from the donkey car utility doc The donkey command is created when you install the donkeycar Python package. This is a Python script that adds some important functionality. The operations here are vehicle independent, and should work on any hardware configuration. 4.1 Create Car This command creates a new dir which will contain the files needed to run and train your robot. Usage: donkey createcar --path &lt;dir&gt; [--overwrite] [--template &lt;donkey2&gt;] This command may be run from any dir Run on the host computer or the robot It uses the --path as the destination dir to create. If .py files exist there, it will not overwrite them, unless the optional --overwrite is used. The optional --template will specify the template file to start from. For a list of templates, see the donkeycar/templates dir 4.2 Find Car This command attempts to locate your car on the local network using nmap. Usage: donkey findcar Run on the host computer Prints the host computer IP address and the car IP address if found Requires the nmap utility: sudo apt install nmap 4.3 Calibrate Car This command allows you to manually enter values to interactively set the PWM values and experiment with how your robot responds. See also more information. Usage: donkey calibrate --channel &lt;0-15 channel id&gt; Run on the host computer Opens the PWM channel specified by --channel Type integer values to specify PWM values and hit enter Hit Ctrl + C to exit 4.4 Clean data in Tub Opens a web server to delete bad data from a tub. Usage: donkey tubclean &lt;folder containing tubs&gt; Run on pi or host computer. Opens the web server to delete bad data. Hit Ctrl + C to exit 4.5 Make Movie from Tub This command allows you to create a movie file from the images in a Tub. Usage: donkey makemovie &lt;tub_path&gt; [--out=&lt;tub_movie.mp4&gt;] [--config=&lt;config.py&gt;] [--model=&lt;model path&gt;] [--model_type=(linear|categorical|rnn|imu|behavior|3d)] [--start=0] [--end=-1] [--scale=2] [--salient] Run on the host computer or the robot Uses the image records from --tub dir path given Creates a movie given by --out. Codec is inferred from file extension. Default: tub_movie.mp4 Optional argument to specify a different config.py other than default: config.py Optional model argument will load the keras model and display prediction as lines on the movie model_type may optionally give a hint about what model type we are loading. Categorical is default. optional --salient will overlay a visualization of which pixels excited the NN the most optional --start and/or --end can specify a range of frame numbers to use. scale will cause ouput image to be scaled by this amount Examples: (donkeyV3) Uwes-MBP:mycarV3 uwesterr$ donkey makemovie --tub=/Users/uwesterr/mycarV3/data/tubVaihingenIIIProcessed --out=VaihingenTubMovieV3.mp4 --config=config.py --model=/Users/uwesterr/mycarV3/vaihingenIII.h5 --type=linear --start=0 --end=1000 --salient Results in the following movie The animations within the video are: Green lines are the steering predictions as per defined model Visualization of which pixels excited the NN the most 4.6 Check Tub This command allows you to see how many records are contained in any/all tubs. It will also open each record and ensure that the data is readable and intact. If not, it will allow you to remove corrupt records. Usage: donkey tubcheck &lt;tub_path&gt; [--fix] donkey tubcheck /Users/uwesterr/mycarV3/data/tubVaihingenIIIProcessed Run on the host computer or the robot It will print summary of record count and channels recorded for each tub It will print the records that throw an exception while reading The optional --fix will delete records that have problems 4.7 Histogram This command will show a pop-up window showing the histogram of record values in a given tub. Usage: donkey tubhist &lt;tub_path&gt; --rec=&lt;&quot;user/angle&quot;&gt; Run on the host computer When the --tub is omitted, it will check all tubs in the default data dir 4.8 Plot Predictions This command allows you plot steering and throttle against predictions coming from a trained model. Usage: donkey tubplot &lt;tub_path&gt; [--model=&lt;model_path&gt;] This command may be run from ~/mycar dir Run on the host computer Will show a pop-up window showing the plot of steering values in a given tub compared to NN predictions from the trained model When the --tub is omitted, it will check all tubs in the default data dir 4.9 Simulation Server This command allows you serve steering and throttle controls to a simulated vehicle using the Donkey Simulator. Usage: donkey sim --model=&lt;model_path&gt; [--type=&lt;linear|categorical&gt;] [--top_speed=&lt;speed&gt;] [--config=&lt;config.py&gt;] This command may be run from ~/mycar dir Run on the host computer Uses the model to make predictions based on images and telemetry from the simulator --type can specify whether the model needs angle output to be treated as categorical Top speed can be modified to ascertain stability at different goal speeds 4.10 Colab training on raspi bundel the data for easy retrieval (env) pi@donkeypi_uwe:~/mycar/data $ tar -czf tubRp.tar.gz tub* fist we need to get the data in a format to upload to our google mydrive as a tar zip tar -czf tubTest.tar.gz tub* more info on how to train in colab is given in the notebook itself https://colab.research.google.com/github/robocarstore/donkey-car-training-on-google-colab/blob/master/Donkey_Car_Training_using_Google_Colab.ipynb#scrollTo=0ShFSsaewLCT 4.11 Transfer learning Tawn Kramer wrote a little tool to do transfer learning which can be downloaded from https://gist.github.com/tawnkramer/5fa63a805e045504c4e56004e4f93e42 There is a python file train_imagenet.py As a starting point Tawn also offers a .h5 model base_linear.h5 which can be downloaded from https://drive.google.com/open?id=18Qfc_T5fpUmuTqXFZbJZrQhlf2l3S0VK "],
["donkeyCarSoftwareAnalysis.html", "Chapter 5 Donkeycar software analysis 5.1 How to add parts to a vehicle 5.2 Structure of manage.py 5.3 Locate modules on raspi which are being imported by manage.py 5.4 Assignment of WiiU controller sticks to action 5.5 Limit max velocity 5.6 Add LED to indicate recording mode 5.7 Change default recording status to false", " Chapter 5 Donkeycar software analysis In this section the software of the donkeycar project is analysed with emphasis of how to use the software to gather training data and to modify the Keras model 5.1 How to add parts to a vehicle Two brilliant videos by Tawn Kramer describe the Donkeycar framework. The first part describes in detail how to add a part to the vehicle. In short, a part will be added to the vehicle class and for each iteration of the loop the run or the run_threaded method of the added class will be executed. The stacking of the methods are shown in figure 5.1 Figure 5.1: Stacking of methods with from donkeycar.vehicle import Vehicle the Vehicle class is available and is instantiated by V = Vehicle() 5.1.1 Function V.start() To start vehicle’s main drive loop. V.start() This is the main thread of the vehicle. It starts all the new threads for the threaded parts then starts an infinite loop that runs each part and updates the memory. A Vehicle has memory parts threads Boolean flag “ON” 5.1.1.1 Parameters of V.start() The parameters for V.start() are rate_hz : int (default = 10Hz) The max frequency that the drive loop should run. The actual frequency may be less than this if there are many blocking parts. max_loop_count : int (default = None) Maximum number of loops the drive loop should execute. This is used for testing the all the parts of the vehicle work. 5.1.1.2 Main run loop of V.start() The main run loop of the V.start() function is below. The sleep_time assures that the loop rate is achieved. This can only be guaranteed if the update process duration of the parts does not exceed the loop time. loop_count = 0 while self.on: start_time = time.time() loop_count += 1 self.update_parts() # stop drive loop if loop_count exceeds max_loopcount if max_loop_count and loop_count &gt; max_loop_count: self.on = False sleep_time = 1.0 / rate_hz - (time.time() - start_time) if sleep_time &gt; 0.0: time.sleep(sleep_time) 5.1.2 Function V.update_parts() In the V.update_parts() function the appropriate run function of part are being called. 5.1.2.1 Parameters of V.parts() There are no parameters 5.1.2.2 Loop over all parts The following code will be looped over for all parts for entry in self.parts: # don&#39;t run if there is a run condition that is False run = True if entry.get(&#39;run_condition&#39;): run_condition = entry.get(&#39;run_condition&#39;) run = self.mem.get([run_condition])[0] # print(&#39;run_condition&#39;, entry[&#39;part&#39;], entry.get(&#39;run_condition&#39;), run) if run: p = entry[&#39;part&#39;] # get inputs from memory inputs = self.mem.get(entry[&#39;inputs&#39;]) # run the part if entry.get(&#39;thread&#39;): outputs = p.run_threaded(*inputs) else: outputs = p.run(*inputs) # save the output to memory if outputs is not None: self.mem.put(entry[&#39;outputs&#39;], outputs) It checks whether the part has inputs and passes the input to p.run_threaded if the part is threaded p.run if the part is not threaded If there are outputs the are passed to the output of the part using the name of the keys supplied by the call. The names of the inputs and outputs are defined when the parts are added via the V.add()function 5.1.3 Function V.add Method to add a part to the vehicle drive loop. The names of parts in- and outputs are defined by the parameters of the V.addfunction 5.1.3.1 Parameters of V.add() inputs : list Channel names to get from memory. outputs : list Channel names to save to memory. threaded : Boolean If a part should be run in a separate thread. run_condition: Boolean If a part should be run at all. 5.1.3.2 Example to add Web controller The web controller is first instantiated and then added using the V.add()function. The names of in- and outputs are supplied as well as the indicator that the controller is threaded. More on threaded parts in section 5.1.4 ctr = LocalWebController(use_chaos=use_chaos) V.add(ctr, inputs=[&#39;cam/image_array&#39;], outputs=[&#39;user/angle&#39;, &#39;user/throttle&#39;, &#39;user/mode&#39;, &#39;recording&#39;], threaded=True) 5.1.3.3 Example add Bluetooth Controller To add a Bluetooth controller add the following lines to manage.py in the drive method from donkeypart_bluetooth_game_controller import BluetoothGameController ctl = BluetoothGameController() V.add(ctl, inputs=[&#39;cam/image_array&#39;], outputs=[&#39;user/angle&#39;, &#39;user/throttle&#39;, &#39;user/mode&#39;, &#39;recording&#39;], threaded=True) 5.1.4 Concept of threads vs non-threaded parts Updating of the parts is run in the loop of the V.update(). To avoid that parts which take long to update blocks the loop they can be assigned to a dedicated thread. Whenever the a threaded part is updated the run_threadedmethod is run. For a camera the run_threaded is def run_threaded(self): return self.frame which returns the frame available at the time, no delay being imposed on the loop. 5.1.4.1 The update function Images are taken constantly as fast as possible by the method update as shown below def update(self): # keep looping infinitely until the thread is stopped for f in self.stream: # grab the frame from the stream and clear the stream in # preparation for the next frame self.frame = f.array self.rawCapture.truncate(0) # if the thread indicator variable is set, stop the thread if not self.on: break 5.1.4.2 The shutdown function To stop the thread a shutdownfunction needs to be defined def shutdown(self): # indicate that the thread should be stopped self.on = False print(&#39;stoping PiCamera&#39;) time.sleep(.5) self.stream.close() self.rawCapture.close() self.camera.close() 5.2 Structure of manage.py manage.py contains scripts to drive a donkey 2 car and train a model for it. The structure of manage.pyis well explained by Tawn Kramer in Donkeycar Parts Overview Pt 2/2 5.2.1 Creating manage.py manage.pyis created when the command donkey createcar --path ~/d2 is run and is a copy of /templates/donkey2.py If you want to add other templates this can be done by adding the name of the template without the extension “.py”. donkey createcar --path ~/d2 --template tk Then tk.pywill be copied into manage.py 5.2.2 Starting manage.py In order to start manage.py without the need to write pyhton chmode +x manage.py ./manage.py The line #!/usr/bin/env python3 makes sure that the script is run with pyhton3. 5.2.3 Parsing the command line parameters docopt is a Command-line interface description language with which the following options are parsed. The parsed inputs are then used by if __name__ == &#39;__main__&#39;: args = docopt(__doc__) cfg = dk.load_config() if args[&#39;drive&#39;]: drive(cfg, model_path=args[&#39;--model&#39;], use_chaos=args[&#39;--chaos&#39;]) elif args[&#39;train&#39;]: tub = args[&#39;--tub&#39;] new_model_path = args[&#39;--model&#39;] base_model_path = args[&#39;--base_model&#39;] cache = not args[&#39;--no_cache&#39;] train(cfg, tub, new_model_path, base_model_path) Note, the if __name__ == '__main__': part is only run if the code is called as first script. The __doc__ string is the text block at the beginning of the script. Parameters in () are mandatory, parameters in [ ] are optional. Usage: manage.py (drive) [--model=&lt;model&gt;] [--js] [--chaos] manage.py (train) [--tub=&lt;tub1,tub2,..tubn&gt;] (--model=&lt;model&gt;) [--base_model=&lt;base_model&gt;] [--no_cache] Options: -h --help Show this screen. --tub TUBPATHS List of paths to tubs. Comma separated. Use quotes to use wildcards. ie &quot;~/tubs/*&quot; --chaos Add periodic random steering when manually driving 5.2.4 Function drive Construct a working robotic vehicle from many parts. Each part runs as a job in the Vehicle loop, calling either it’s run or run_threaded method depending on the constructor flag threaded. All parts are updated one after another at the frame rate given in cfg.DRIVE_LOOP_HZ assuming each part finishes processing in a timely manner. Parts may have named outputs and inputs. The framework handles passing named outputs to parts requesting the same named input. 5.2.4.1 Lambda class Lambda wraps a function into a donkey part, i.e. it will be called for every update loop. class Lambda: &quot;&quot;&quot; Wraps a function into a donkey part. &quot;&quot;&quot; def __init__(self, f): &quot;&quot;&quot; Accepts the function to use. &quot;&quot;&quot; self.f = f def run(self, *args, **kwargs): return self.f(*args, **kwargs) def shutdown(self): return The function is defined as usual put then apply Lambdato defined function V.add output of Lambda # See if we should even run the pilot module. # This is only needed because the part run_condition only accepts boolean def pilot_condition(mode): if mode == &#39;user&#39;: return False else: return True pilot_condition_part = Lambda(pilot_condition) V.add(pilot_condition_part, inputs=[&#39;user/mode&#39;], outputs=[&#39;run_pilot&#39;]) The output run_pilotis used to define whether or not klis run. V.add(kl, inputs=[&#39;cam/image_array&#39;], outputs=[&#39;pilot/angle&#39;, &#39;pilot/throttle&#39;], run_condition=&#39;run_pilot&#39;) 5.3 Locate modules on raspi which are being imported by manage.py Where on the Raspberry Pi are the modules located which are being imported by the python script manage.py from docopt import docopt import donkeycar as dk from donkeycar.parts.camera import PiCamera from donkeycar.parts.transform import Lambda from donkeycar.parts.keras import KerasLinear from donkeycar.parts.actuator import PCA9685, PWMSteering, PWMThrottle from donkeycar.parts.datastore import TubGroup, TubWriter from donkeycar.parts.web_controller import LocalWebController from donkeycar.parts.clock import Timestamp from donkeypart_bluetooth_game_controller import BluetoothGameController donkeycar is found at the raspi (env) pi@donkeypi_uwe:~ $ find . -name donkeycar ./env/lib/python3.5/site-packages/donkeycar the contents of this folder is shown below (env) pi@donkeypi_uwe:~ $ cd ./env/lib/python3.5/site-packages/donkeycar (env) pi@donkeypi_uwe:~/env/lib/python3.5/site-packages/donkeycar $ ls config.py __init__.py log.py management memory.py parts __pycache__ templates tests util vehicle.py which are the same files and folders which are located at the official GitHub page of DonkeyCar project The donkeypart_bluetooth_game_controller was installed during the set up of the raspi for the project as described in section 2.7.1 5.4 Assignment of WiiU controller sticks to action The mapping from button codes to button names is defined in donkeypart_bluetooth_game_controller/wiiu_config.yml device_search_term: &#39;nintendo&#39; #Map the button codes to the button names button_map: 305: &#39;A&#39; 304: &#39;B&#39; 307: &#39;X&#39; 308: &#39;Y&#39; 312: &#39;LEFT_BOTTOM_TRIGGER&#39; 310: &#39;LEFT_TOP_TRIGGER&#39; 313: &#39;RIGHT_BOTTOM_TRIGGER&#39; 311: &#39;RIGHT_TOP_TRIGGER&#39; 317: &#39;LEFT_STICK_PRESS&#39; 318: &#39;RIGHT_STICK_PRESS&#39; 314: &#39;SELECT&#39; 315: &#39;START&#39; 0: &#39;LEFT_STICK_X&#39; 1: &#39;LEFT_STICK_Y&#39; 3: &#39;RIGHT_STICK_X&#39; 4: &#39;RIGHT_STICK_Y&#39; 547: &#39;PAD_RIGHT&#39; 546: &#39;PAD_LEFT&#39; 544: &#39;PAD_UP&#39; 548: &#39;PAD_DOWN&#39; joystic_max_value: 1280 which is then used in donkeypart_bluetooth_game_controller/part.py In the class definition BluetoothGameController def __init__(self, event_input_device=None, config_path=None, device_search_term=None, verbose=False): . . . self.func_map = { &#39;LEFT_STICK_X&#39;: self.update_angle, &#39;LEFT_STICK_Y&#39;: self.update_throttle, &#39;B&#39;: self.toggle_recording, &#39;A&#39;: self.toggle_drive_mode, &#39;PAD_UP&#39;: self.increment_throttle_scale, &#39;PAD_DOWN&#39;: self.decrement_throttle_scale, } It seems to be odd to have steering and throttle on one stick, to assign throttle to the right stick change donkeypart_bluetooth_game_controller/part.py to self.func_map = { &#39;LEFT_STICK_X&#39;: self.update_angle, &#39;RIGHT_STICK_Y&#39;: self.update_throttle, to make the controller more sensitive it might be a good idea to limit the max joystick value in donkeypart_bluetooth_game_controller/wiiu_config.yml to the max value needed which is according to 2.5.4 5.5 Limit max velocity To avoid that the car becomes too fast there are two variables in myconfig.py 5.5.1 Limit max velocity for joystick input To limit the max velocity due to joystick input set in myconfig.py the variable JOYSTICK_MAX_THROTTLE to a value of your liking JOYSTICK_MAX_THROTTLE = 0.5 #this scalar is multiplied with the -1 to 1 throttle value to limit the maximum throttle. This can help if you drop the controller or just don&#39;t need the full speed available. 5.5.2 Limit max velocity for ai pilot To limit the max velocity for ai pilot set in myconfig.py the variable AI_THROTTLE_MULT to a value of your liking AI_THROTTLE_MULT = 1.0 # this multiplier will scale every throttle value for all output from NN models 5.6 Add LED to indicate recording mode Recording of images can be activated pressing button “B” on the Wii controller. Assignment of buttons to actions can be found at 5.4. To indicate if recording is active a LED can be used. A function can be added to the update loop of the vehicle class using the lambda class 5.2.4.1. Channel 0 and 1 are already taken, channel 3 is available. So the LED controller channel can be set by Led_controller = PCA9685(3) The function and wrapping it by the lambda class is below # Switch LED depending on recording video or not. def led_switch(recording): Led_controller.set_pulse(4095*recording) return led_switch_part = Lambda(led_switch) V.add(led_switch_part, inputs=[&#39;recording&#39;], outputs=[]) 5.6.1 Wiring of LED to PWM/Servo HAT How to wire the LED to the PWM/Servo hat is shown in 5.2 Figure 5.2: Wiring diagram of LED to PWM/Servo hat Note: you don’t need to use a resistor to limit current through the LED as the HAT and Bonnet will limit the current to around 10mA. More details on how to connect the LED are given in (https://cdn-learn.adafruit.com/downloads/pdf/adafruit-16-channel-pwm-servo-hat-for-raspberry-pi.pdf) in chapter “Python Wiring” 5.7 Change default recording status to false To avoid unnecessary images in the tub folder the default recording status is set to falseby changing in the file ~/donkeypart_bluetooth_game_controller/donkeypart_bluetooth_game_controller/part.py using the editor nano nano part.py the line self.recording_toggle = cycle([True, False]) to self.recording_toggle = cycle([False, True]) so the cycle starts with False and can be toggled by pressing the button B on the Wii U controller. How to indicate the recording mode with an LED is described in 5.6 "],
["installation-of-tawn-kramers-donkeycar-version-by-rainer-bareiß.html", "Chapter 6 Installation of Tawn Kramers donkeycar version (By Rainer Bareiß) 6.1 Environment 6.2 sudo raspi-config 6.3 etc/wpa_supllicant/wpa_supplicant.conf 6.4 installs 6.5 install donkey on pi 6.6 5. config.piy 6.7 bluetooth 6.8 8. wii u controller 6.9 autostart donkeycar 6.10 Steering Calibration 6.11 drive", " Chapter 6 Installation of Tawn Kramers donkeycar version (By Rainer Bareiß) following Tawn Kramer https://github.com/tawnkramer/donkey/blob/master/docs/guide/install_software.md 6.1 Environment 1. conda create --name donkey_3tk4 tensorflow-gpu==1.10.0 2. source activate donkey_3tk4 3. git clone https://github.com/tawnkramer/donkey.git 4. pip install -e ./donkey 5. conda install -c conda-forge keras==2.2.2 activate: source activate donkey_3tk4 get installed software: conda env export &gt; environment.yml content of environment.yml 6.2 sudo raspi-config a. generate DE locales b. set german keyboard layout c. enable i2c, spi d. expand SDcard e. set hostname = siliconpi2 6.3 etc/wpa_supllicant/wpa_supplicant.conf a. setup SSID &amp; password b. (env) pi@siliconpi2:/etc/wpa_supplicant $ sudo more wpa_supplicant.conf country=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=&quot;XXXXXXXXXXX&quot; psk=&quot;dontstorepasswords&quot; id_str=&quot;home&quot; } 6.4 installs a. sudo apt-get install vim b. sudo apt-get upgrade c. sudo apt-get update d. sudo rpi-update 6.5 install donkey on pi follow: https://github.com/tawnkramer/donkey/blob/master/docs/guide/install_software.md 6.6 5. config.piy #STEERING STEERING_CHANNEL = 1 STEERING_LEFT_PWM = 480 #500 #420 STEERING_RIGHT_PWM = 290 #360 #THROTTLE THROTTLE_CHANNEL = 0 THROTTLE_FORWARD_PWM = 500 #400 THROTTLE_STOPPED_PWM = 360 THROTTLE_REVERSE_PWM = 310 6.7 bluetooth sudo bluetoothctl agent on power on scan on connect &lt;Mac&gt; trust &lt;Mac&gt; 6.8 8. wii u controller setup joystick: https://github.com/tawnkramer/donkey/blob/master/docs/parts/controllers.md#physical-joystick-controller 6.9 autostart donkeycar https://gist.github.com/r7vme/9159c52ec72660d8ace02793a5cee788 # Allows to automatically start donkey car on boot. # # 1. Put contents to /etc/systemd/system/donkey.service # 2. sudo systemctl daemon-reload # 3. sudo systemctl start donkey # 4. sudo journactl -u donkey # # TIP: Grab logs via ssh with # # ssh pi@d2.local &quot;sudo journalctl -u donkey -f&quot; # [Unit] Description=Donkey car [Service] Restart=always ExecStart=/bin/su - pi bash -c &quot;python -u d2/manage.py drive --model /home/pi/d2/models/mypilot&quot; [Install] WantedBy=multi-user.target 6.10 Steering Calibration Make sure your car is off the ground to prevent a runaway situation. 1. Turn on your car. 2. Find the servo cable on your car and see what channel it&#39;s plugged into the PCA board. It should be 1 or 0. 3. Run donkey calibrate --channel &lt;your_steering_channel&gt; 4. Enter 360 and you should see the wheels on your car move slightly. If not enter 400 or 300. 5. Next enter values +/- 10 from your starting value to find the PWM setting that makes your car turn all the way left and all the way right. Remember these values. 6. Enter these values in config.py script as STEERING_RIGHT_PWM and STEERING_LEFT_PWM. LEFT =500 RIGHT = 290 6.11 drive siliconpi2: cd d2 python manage.py drive "],
["driving-training-following-tawn-kramer-by-rainer-bareiß.html", "Chapter 7 Driving &amp; Training following Tawn Kramer (By Rainer Bareiß) 7.1 Environment 7.2 Read data from pi 7.3 Check data raw &amp; prepared 7.4 3. Clean data 7.5 Make movie 7.6 tubhist 7.7 Train Data 7.8 Plot data against model 7.9 Run in simulator 7.10 Copy model to pi 7.11 Run model on pi 7.12 11. Watch pi camera 7.13 Reinfocement Learning", " Chapter 7 Driving &amp; Training following Tawn Kramer (By Rainer Bareiß) following Tawn Kramer https://github.com/tawnkramer/donkey/blob/master/docs/guide/get_driving.md 7.1 Environment activate: source activate donkey_3tk4 get installed software: conda env export &gt; environment.yml content of environment.yml 7.2 Read data from pi (carnd-tf16) rainer@neuron:/media/rainer/_data/20-data/M3-robocar_training/20190112-Shackspace/raw$ rsync -r pi@siliconpi1:~/20190112-tub-shack/tub? . 7.3 Check data raw &amp; prepared (donkey_3tk4) rainer@neuron:/media/rainer/_data/20-data/M3-robocar_training/20190112-Shackspace$ donkey tubcheck raw/tub1 raw/tub2 raw/tub3 raw/tub4 ` (carnd-tf16) rainer@neuron:/media/rainer/_data/20-data/M3-robocar_training/20190223-Shackspace$ donkey tubcheck tub* 7.4 3. Clean data (carnd-tf16) rainer@neuron:/media/rainer/_data/20-data/M3-robocar_training$ donkey tubclean 20190223-Shackspace/ using donkey version: 2.5.7 ... Listening on 8886... 7.5 Make movie (carnd-tf16) rainer@neuron:/media/rainer/_data/20-data/M3-robocar_training/20190112-Shackspace$ donkey makemovie --tub=raw/tub1 --config=raw/config.py --out=tub1.mp4 (carnd-tf16) rainer@neuron:/media/rainer/_data/20-data/M3-robocar_training/20190223-Shackspace$ scp pi@siliconpi2:~/d2/config.py . donkey makemovie --tub=raw/tub_36_19-02-23 --config=config.py --out=tub_36_19-02-23.mp4 7.6 tubhist (donkey_3tk4) rainer@neuron:~/dev/25-donkey_3tk$ donkey tubhist \\ --tub /media/rainer/_data/20-data/M3-robocar_training/20190112-Shackspace/prepared/* tubhist_shack (donkey_3tk4) rainer@neuron:~/dev/25-donkey_3tk$ donkey tubhist \\ --tub /media/rainer/_data/20-data/M3-robocar_training/20190126-TawnKramer/combined_donkey_tub_data/* tubhist_tawn (donkey_3tk4) rainer@neuron:/media/rainer/_data/20-data/M3-robocar_training/20190223-Shackspace$ donkey tubhist --tub prepared/* tubhist_tub_36_19-02-23 7.7 Train Data (donkey_3tk4) rainer@neuron:~/dev/25-donkey_3tk/mycar/$ python manage.py train \\ --tub=/media/rainer/_data/20-data/M3-robocar_training/20190112-Shackspace/prepared/* \\ --model=/home/rainer/dev/25-donkey_3tk/mycar/models/shack1.h5 model loss for training shack1.h5 (donkey_3tk4) rainer@neuron:~/dev/25-donkey_3tk/mycar$ python train.py \\ --tub=/media/rainer/_data/20-data/M3-robocar_training/20190223-Shackspace/prepared/* \\ --model=/home/rainer/dev/25-donkey_3tk/mycar/models/shack2.h5 model loss for training shack2.h5 Attention: What happened during training above? Hint, mind the accuracy of testing data. 7.8 Plot data against model cd (donkey_3tk4) rainer@neuron:~/dev/25-donkey_3tk/mycar donkey tubplot \\ --tub=/media/rainer/_data/20-data/M3-robocar_training/20190112-Shackspace/prepared/tub2_c \\ --model=/home/rainer/dev/25-donkey_3tk/mycar/models/shack1.h5 tub2_c tub3_c tub4_c (donkey_3tk4) rainer@neuron:~/dev/25-donkey_3tk/mycar$ donkey tubplot \\ --tub=/media/rainer/_data/20-data/M3-robocar_training/20190112-Shackspace/prepared/tub4_c \\ --model=/home/rainer/dev/25-donkey_3tk/mycar/models/shack2.h5 tub4_cvs shack2 7.9 Run in simulator (donkey_3tk4) rainer@neuron:~/dev/25-donkey_3tk/donkey_gym/examples/supervised_learning$ # python evaluate.py --model /home/rainer/dev/25-donkey_3tk/mycar/models/tawn_shark_cl.h5 python evaluate.py --model /home/rainer/dev/25-donkey_3tk/mycar/models/shack1.h5 (donkey_3tk4) rainer@neuron:~/dev/25-donkey_3tk/DonkeySimLinux$ ./donkey_sim.x86_64 7.10 Copy model to pi (donkey_3tk) rainer@neuron:~/dev/25-donkey_3tk$ scp /home/rainer/dev/25-donkey_3tk/mycar/models/shack1.h5 pi@siliconpi2:~/d2/models 7.11 Run model on pi (env) pi@siliconpi1:~/mycar_autopilot $ python manage.py drive --model=models/shack1.h5 python manage.py drive --model &lt;path/to/model&gt; --js Hit the Select button to toggle between three modes - User, Local Angle, and Local Throttle &amp; Angle. User - User controls both steering and throttle with joystick Local Angle - Ai controls steering, user controls throttle Local Throttle &amp; Angle - Ai controls both steering and throttle When the car is in Local Angle mode, the NN will steer. You must provide throttle. 7.12 11. Watch pi camera (donkey_3tk) rainer@neuron:~/dev/25-donkey_3tk/mycar$ python ../donkey/scripts/remote_cam_view.py --ip=siliconpi2 remote browser: http://siliconpi2:3233 7.13 Reinfocement Learning (donkey_3tk) rainer@neuron:~/dev/25-donkey_3tk/donkey_gym/examples/reinforcement_learning$ python ddqn.py --sim /home/rainer/dev/25-donkey_3tk/DonkeySimLinux/donkey_sim.x86_64 Testing ppo trained net (donkey_3tk) rainer@neuron:~/dev/25-donkey_3tk/donkey_gym/examples/reinforcement_learning$ python ppo_train.py --test --sim /home/rainer/dev/25-donkey_3tk/DonkeySimLinux/donkey_sim.x86_64 python ppo_train.py --sim /home/rainer/dev/25-donkey_3tk/DonkeySimLinux/donkey_sim.x86_64 --env_name donkey-generated-roads-v0 python ppo_train.py --sim /home/rainer/dev/25-donkey_3tk/DonkeySimLinux/donkey_sim.x86_64 --env_name donkey-generated-roads-v0 --headless 1 python ppo_train.py --sim /home/rainer/dev/25-donkey_3tk/DonkeySimLinux/donkey_sim.x86_64 --env_name donkey-generated-roads-v0 --headless 1 --multi (donkey_3tk) rainer@neuron:~/dev/25-donkey_3tk$ python donkey_gym/examples/supervised_learning/evaluate.py --model=siliconpi_neuron1_donkey2.h5 using donkey v2.5.0t ... 2019-01-15 18:42:56.116375: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA binding to (&#39;0.0.0.0&#39;, 9091) got a new client (&#39;127.0.0.1&#39;, 40308) unknown message type scene_selection_ready connection dropped got a new client (&#39;127.0.0.1&#39;, 40310) unknown message type car_loaded fps 20.194938775588813 fps 19.99812811253143 fps 20.00077822852299 fps 20.002373023227776 fps 19.99857054437418 fps 20.001034790170067 fps 19.998124298548927 fps 20.001525041510646 fps 20.000947997203312 fps 19.998230137103345 "],
["robocarLogBook.html", "Chapter 8 RoboCar Logbook 8.1 15-02-2018 8.2 Build sunfounder car 17-02-2018 8.3 Viewing a list of available Python versions 8.4 Run calibration 8.5 MJPG-streamer Installation", " Chapter 8 RoboCar Logbook The logbook shall be the place to log activities so that future me will understand what present me was doing and maybe even why. 8.1 15-02-2018 Already installed donkey software on mac at /Users/uwesterr/miniconda3/envs/donkey according to http://docs.donkeycar.com/guide/install_software/#install-donkeycar-on-mac 8.1.1 donkey findcar the command donkey findcar leads to sudo: nmap: command not found therefore nmap shall be installed using sudo apt install nmap this leads to error message Unable to locate an executable at \"/Library/Java/JavaVirtualMachines/jdk1.8.0_05.jdk/Contents/Home/bin/apt\" (-1) 8.2 Build sunfounder car 17-02-2018 took about 4h, very good instructions at https://www.sunfounder.com/learn/download/U21hcnRfVmlkZW9fQ2FyX2Zvcl9SYXNwYmVycnlfUGkucGRm/dispi 8.2.1 install software 18-02-2018 download RASPBIAN STRETCH LITE from https://www.raspberrypi.org/downloads/raspbian/ use Etcher to flash SD card for raspi and put SD card in adapter to put into card reader of mac book Create a blank file ssh under the /boot directory to enable remote login and delete the suffix in the file name. Create a WiFi configuration file wpa_supplicant.conf under /boot according to http://www.linux-ratgeber.de/wlan-verbindungsdaten-einrichten-ueber-die-wpa_supplicant-conf/ ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=&quot;WLAN-NAME_1&quot; psk=&quot;WLAN-SCHLUESSEL_1&quot; proto=RSN scan_ssid=1 key_mgmt=WPA-PSK pairwise=CCPM group=TKIP } 8.2.1.1 boot up raspi username: pi password: raspberry change password type passwd and follow instructions change to german keyboard layout sudo nano /etc/default/keyboard find where it says XKBLAYOUT=”gb” to XKBLAYOUT=”de” and change the gb to the two letter code for german pipe symbol | =&gt; alt option + &lt; Get Source Code sudo apt-get install git git clone https://github.com/sunfounder/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi.git Install python-dev, python-smbus sudo apt-get update sudo apt-get upgrade sudo apt-get install python-dev sudo apt-get install python-smbus Setup I2C Port sudo raspi-config could not get wlan to work install now RASPBIAN STRETCH WITH DESKTOP instead RASPBIAN STRETCH LITE hostnam uwesCar do the follwoing steps again Get Source Code sudo apt-get install git git clone https://github.com/sunfounder/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi.git Install python-dev, python-smbus sudo apt-get update sudo apt-get upgrade sudo apt-get install python-dev sudo apt-get install python-smbus Setup I2C Port done this time in pixel desktop Start calibration cd ~/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi/server sudo python cali_server.py set wlan sudo nano /etc/wpa_supplicant/wpa_supplicant.conf sudo nano /etc/wpa_supplicant/wpa_supplicant.conf Run cali_client cali_client.py is written in python2, i got python 3 installed changed cali_client.py added parenthesis for print statements Python 3 calls it tkinter not Tkinter. run sudo python cali_client.py leads to Exception in Tkinter callback Traceback (most recent call last): File &quot;/Users/uwesterr/miniconda3/lib/python3.6/tkinter/__init__.py&quot;, line 1699, in __call__ return self.func(*args) File &quot;cali_client.py&quot;, line 56, in left_reverse tcpCliSock.send(left_cmd) TypeError: a bytes-like object is required, not &#39;str&#39; therefore build an environment with python 2 conda create --name sunfounder gives Package plan for installation in environment /Users/uwesterr/miniconda3/envs/sunfounder: To activate this environment, use: &gt; source activate sunfounder To deactivate an active environment, use: &gt; source deactivate python –version Python 3.6.0 :: Continuum Analytics, Inc. 8.3 Viewing a list of available Python versions To list the versions of Python that are available to install, in your Terminal window or an Anaconda Prompt, run: conda search python This lists all packages whose names contain the text python. To list only the packages whose full name is exactly python, add the –full-name option. In your Terminal window or an Anaconda Prompt, run: conda search --full-name python conda search –full-name python build environment with python 2.7 conda create -n sunfounderPy27 python=2.7 anaconda To activate this environment, use: source activate sunfounderPy27 To deactivate an active environment, use: source deactivate Uwes-MBP:~ uwesterr$ source activate sunfounderPy27 (sunfounderPy27) Uwes-MBP:~ uwesterr$ python –version Python 2.7.13 :: Anaconda 4.4.0 (x86_64) raspberry ip 192.168.178.67 8.4 Run calibration on raspbeery cd ~/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi/server sudo python cali_server.py on mac source activate sunfounderPy27 cd /Users/uwesterr/CloudProjectsUnderWork/ProjectsUnderWork/RoboCar/sunfounder/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi/client sudo python cali_client.py 8.5 MJPG-streamer Installation Installation Plug the USB camera into Raspberry Pi, and run the command lsusb. The GEMBIRD represents the USB camera; since it is printed on the screen, it indicates the system has recognized the camera. lsusb resutls in Bus 001 Device 004: ID 1908:2310 GEMBIRD Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Check whether the driver for the camera works normally: ls /dev/vid* ls /dev/vid* results in /dev/video0 install the following software sudo apt-get install subversion sudo apt-get install libv4l-dev sudo apt-get install libjpeg8-dev sudo apt-get install imagemagick Compile the source code of MJPG-streamer: cd /home/pi/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi/mjpg-streamer/mjpg-streamer sudo make USE_LIBV4L2=true clean all Install: sudo make DESTDIR=/usr install 8.5.1 Testing Operation on Raspberry Pi Run the program: sudo sh start.sh on Mac Type in the following address at the address bar of your browser (Firefox is recommended): http://192.168.178.67:8080/stream.html "],
["get-on-the-road.html", "Chapter 9 Get on the Road! 9.1 install carnd-term1 9.2 Install Xbox 360 controller 9.3 Implement Xbox controller as input in client_App.py 9.4 Get IP adress of raspi in shack 9.5 Store snapshots of video stream on local computer", " Chapter 9 Get on the Road! Use two terminals for raspberry first teminal run jgp streamer cd /home/pi/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi/mjpg-streamer/mjpg-streamer sudo sh start.sh second terminal run tcp_server.py: cd ~/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi/server sudo python tcp_server.py on mac terminal in environment sunfounderPy27 Type in the following address at the address bar of your browser (Firefox is recommended): http://192.168.178.67:8080/stream.html source activate sunfounderPy27 or if openCV is needed source activate carnd-term1 9.1 install carnd-term1 git clone https://github.com/udacity/CarND-Term1-Starter-Kit.git cd CarND-Term1-Starter-Kit conda env create -f environment.yml conda install -c anaconda tk cd /Users/uwesterr/CloudProjectsUnderWork/ProjectsUnderWork/RoboCar/sunfounder/Sunfounder_Smart_Video_Car_Kit_for_RaspberryPi/client sudo python client_App.py 9.2 Install Xbox 360 controller install Xbox 360 controller drive on mac https://github.com/360Controller/360Controller/releases NOTE!!! it seems necessary to have the controller plugged into the USB port during boot up According to http://www.philipzucker.com/python-xbox-controller-mac/ Install pygame python -m pip install -U pygame --user or brew upgrade sdl sdl_image sdl_mixer sdl_ttf portmidi python3.6 -m venv anenv . ./anenv/bin/activate pip install https://github.com/pygame/pygame/archive/master.zip create a jupyter notebook “xboxControllerOnMac.ipynb”\" with import pygame import sys import time import socket import cPickle as pickle UDP_IP = &quot;127.0.0.1&quot; UDP_PORT = 5005 MESSAGE = &quot;Hello, World!&quot; print &quot;UDP target IP:&quot;, UDP_IP print &quot;UDP target port:&quot;, UDP_PORT print &quot;message:&quot;, MESSAGE sock = socket.socket(socket.AF_INET, # Internet socket.SOCK_DGRAM) # UDP pygame.init() pygame.joystick.init() clock = pygame.time.Clock() print pygame.joystick.get_count() _joystick = pygame.joystick.Joystick(0) _joystick.init() gives: UDP target IP: 127.0.0.1 UDP target port: 5005 message: Hello, World! 1 watch out for the “1” which indicates that the controller was identified while 1: for event in pygame.event.get(): if event.type == pygame.JOYBUTTONDOWN: print(&quot;Joystick button pressed.&quot;) print event if event.type == pygame.JOYAXISMOTION: #print _joystick.get_axis(0) #print event if event.axis == 0: # this is the x axis print event.value if event.axis == 5: # right trigger print event.value xdir = _joystick.get_axis(0) rtrigger = _joystick.get_axis(5) #deadzone if abs(xdir) &lt; 0.2: xdir = 0.0 if rtrigger &lt; -0.9: rtrigger = -1.0 MESSAGE = pickle.dumps([xdir,rtrigger]) sock.sendto(MESSAGE, (UDP_IP, UDP_PORT)) clock.tick(30) when using controller the following output was created 0.00781273842586 -1.00003051851 Joystick button pressed. &lt;Event(10-JoyButtonDown {‘joy’: 0, ‘button’: 6})&gt; 0.0 0.00781273842586 Joystick button pressed. &lt;Event(10-JoyButtonDown {‘joy’: 0, ‘button’: 12})&gt; 0.0312509537034 9.3 Implement Xbox controller as input in client_App.py In client_App.py the part for driving forward extracted from Tkinter import * from socket import * # Import necessary modules ctrl_cmd = [&#39;forward&#39;, &#39;backward&#39;, &#39;left&#39;, &#39;right&#39;, &#39;stop&#39;, &#39;read cpu_temp&#39;, &#39;home&#39;, &#39;distance&#39;, &#39;x+&#39;, &#39;x-&#39;, &#39;y+&#39;, &#39;y-&#39;, &#39;xy_home&#39;] top = Tk() # Create a top window top.title(&#39;Sunfounder Raspberry Pi Smart Video Car&#39;) HOST = &#39;192.168.178.67&#39; # Server(Raspberry Pi) IP address PORT = 21567 BUFSIZ = 1024 # buffer size ADDR = (HOST, PORT) tcpCliSock = socket(AF_INET, SOCK_STREAM) # Create a socket tcpCliSock.connect(ADDR) # Connect with the server # ============================================================================= # The function is to send the command forward to the server, so as to make the # car move forward. # ============================================================================= def forward_fun(event): print &#39;forward&#39; tcpCliSock.send(&#39;forward&#39;) then keystrokes are binded to the forward function, this needs to be changed to bind Xbox controller values to the function # ============================================================================= # Bind buttons on the keyboard with the corresponding callback function to # control the car remotely with the keyboard. # ============================================================================= top.bind(&#39;&lt;KeyPress-w&gt;&#39;, forward_fun) # Press down key &#39;w&#39; on the keyboard and the car will drive forward. from https://github.com/martinohanlon/XboxController/blob/master/XboxController.py JOYAXISMOTION event.axis event.value 0 - x axis left thumb (+1 is right, -1 is left) 1 - y axis left thumb (+1 is down, -1 is up) 2 - x axis right thumb (+1 is right, -1 is left) 3 - y axis right thumb (+1 is down, -1 is up) 4 - right trigger 5 - left trigger JOYBUTTONDOWN | JOYBUTTONUP event.button A = 0 B = 1 X = 2 Y = 3 LB = 4 RB = 5 BACK = 6 START = 7 XBOX = 8 LEFTTHUMB = 9 RIGHTTHUMB = 10 9.3.1 Make steering proportional to remote control lever position change code from if event.axis == 0: # this is the x axis if event.value &gt; thresSteerHigh: tcpCliSock.send(&#39;right&#39;) if event.value &lt; thresSteerLow: tcpCliSock.send(&#39;left&#39;) to if event.axis == 0: # this is the x axis if event.value &gt; thresSteerHigh: tcpCliSock.send(&#39;right&#39;) angle = int(100*abs(event.value)) data = tmp1 + str(angle) print &#39;sendData = %s&#39; % data tcpCliSock.send(data) # Send the speed data to the server(Raspberry Pi) if event.value &lt; thresSteerLow: tcpCliSock.send(&#39;left&#39;) angle = int(-100*abs(event.value)) data = tmp1 + str(angle) print &#39;sendData = %s&#39; % data tcpCliSock.send(data) # Send the speed data to the server(Raspberry Pi) 9.4 Get IP adress of raspi in shack Check in the router for the IP adress, procedure is dependent on router At shackspace go to http://leases.shack/#/ (only available from the shackspace network) and then connect via ssh pi@ipAdress if you get Uwes-MBP:data uwesterr$ ssh pi@10.42.26.33 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the ECDSA key sent by the remote host is SHA256:aYMRAzv3GpxqJNugz1oTi20m0QKVIVfVxszQkJNbNqg. Please contact your system administrator. Add correct host key in /Users/uwesterr/.ssh/known_hosts to get rid of this message. Offending ECDSA key in /Users/uwesterr/.ssh/known_hosts:20 ECDSA host key for 10.42.26.33 has changed and you have requested strict checking. Host key verification failed. then you have to remove the cached key for donkeypi-uwe (or the old IP adress) on the local machine: ssh-keygen -R donkeypi-uwe 9.5 Store snapshots of video stream on local computer Donkey car stores training data on raspi SD card. In this concept the XBox controller is connected via USB to a laptop (in my case a Mac) and it makes sense to store the traing data on the laptop as well since we anyway will train the NN on that machine. The URL of the stream is: url = “http://10.42.26.33:8080/?action=stream” for a snapshot the URL is url = “http://10.42.26.33:8080/?action=snapshot” check video https://youtu.be/2xcUzXataIk?t=556 for a good explanation of how to receive an IP video stream. Based on that tutorial the follwing code now stores a single frame and shows that frame as well. import cv2 import numpy as np import urllib # based on example in https://www.youtube.com/watch?v=2xcUzXataIk url = &quot;http://192.168.178.67:8080/?action=snapshot&quot; imgNp = np.array(bytearray(imgResp.read()), dtype = np.uint8) img = cv2.imdecode(imgNp,-1) cv2.imshow(&quot;test&quot;,img) cv2.imwrite( &quot;Snapshot.jpg&quot;, img ); cv2.waitKey(10000) 9.5.1 Jie Hou’s alternative Jie has another solution for the same task, see https://drive.google.com/drive/folders/10U8ZTr_2HVnBWrFqvVFGO0UyQn0vCmiE The code is import cv2 import numpy as np try: from urllib.request import urlopen except ImportError: from urllib2 import urlopen print(&#39;# capture image from video #&#39;) stream = urlopen(&#39;http://192.168.0.101:8080/?action=stream&#39;) bytes = bytes() FlagSaveImage = 0 while True: bytes += stream.read(1024) a = bytes.find(b&#39;\\xff\\xd8&#39;) b = bytes.find(b&#39;\\xff\\xd9&#39;) print(&#39; #a: &#39;, a, &#39; ,b: &#39;, b) if a != -1 and b != -1: jpg = bytes[a: b + 2] bytes = bytes[b+2:] image = cv2.imdecode(np.fromstring(jpg, dtype=np.uint8), cv2.IMREAD_COLOR) cv2.imshow(&#39;image&#39;, image) if FlagSaveImage == 0: cv2.imwrite(&#39;test.jpg&#39;, image) FlagSaveImage = 1 if cv2.waitKey(1) == 27: exit(0) "],
["resources.html", "Chapter 10 Resources 10.1 Meetups 10.2 RoboCar projects 10.3 Hardware 10.4 Simulators 10.5 Software", " Chapter 10 Resources In this section interesting resources for different topics are listed 10.1 Meetups 10.1.1 Esslinger Makerspace Projekt: Autonomen RoboCar bauen https://www.meetup.com/Esslingen-Makerspace/ Ob ihr euer eigenes RoboCar entwickeln wollt, oder lieber ein kleines Team bilden wollt, hier seit ihr richtig. 10.1.2 Autonomous Mobility Berlin https://www.meetup.com/autonomous-mobility-berlin/ This is a group for anyone interested and intrigued by Autonomous Mobility, Self-Driving Cars (SDC). Robots. We will cover topics on related technologies - Computer Vision, Deep Learning, Reinforcement learning, evolutionary computation, Sensor Fusion, ROS etc.. 10.2 RoboCar projects 10.2.1 DIY RoboCars https://diyrobocars.com/about/ This is the sister site to DIY Drones and resource/community companion to the DIY Robocars Meetup Group. Created by Chris Anderson of 3DR. 10.2.2 Donkey car http://www.donkeycar.com An opensource DIY self driving platform for small scale cars. RC CAR + Raspberry Pi + Python (tornado, keras, tensorflow, opencv, ….) The documentation of the project includes: Assemble hardware. Install software. Calibrate your car. Start driving. Train an autopilot. Experiment with simulator The code can be found at github 10.2.3 Sunfounder Smart Video Car Kit for Raspberry Pi with Android App https://www.sunfounder.com/robotic-drone/smartcar/smart-video-car-kit/rpi-car.html This is a complete learning kit based on Raspberry Pi with Android App. For better learning, an elaborately-written user manual, code with explanation and thorough schematic diagrams are provided. Available at amazon Sunfounder software is found at github Documentation can be found at https://www.sunfounder.com/learn/category/Smart-Video-Car-V2-0-for-Raspberry-Pi-Pi-Car-V.html 10.2.4 Kuman Professional WIFI Smart Robot Model Car Kit Videokamera for Raspberry Pi 3 Einfache Montage und Bedienung: Das ist ein komplettes WIFI-Lern-Smart-Roboter-Kit mit 8GB-Karte basierend auf Raspberry Pi 3 Controlled by ISO Android App. Für den Betrieb leicht, Code of smart Robot System sind in 8 GB Karte vorinstalliert. Für ein besseres Lernen, eine ausführliche schriftliche Benutzerhandbuch, Code mit Erklärung und schematische Diagramme werden von kuman zur Verfügung gestellt. Available at amazon 10.3 Hardware In this section hardware related information is presented 10.3.1 KOMPONENTENLISTE FÜR EIN FERNGESTEUERTES ROBOTER AUTO https://custom-build-robots.com/raspberry-pi-roboter-auto-komponenten Ingmar Stapel from Munich keeps this list up to date ROBOTER AUTO GEHIRN – RASPBERRY PI 3 MODEL B RASPBERRY PI – KAMERAS IM ÜBERBLICK MOTORTREIBER L298N H-BRÜCKE STEP-DOWN CONVERTER (MEINE EMPFEHLUNG) MOTOREN 10.3.2 Motor control How to control steer and throttle 10.3.2.1 PCA9685 10.3.2.1.1 16-KANAL PCA9685 SERVO KONTROLLER – TEIL 1 EINFÜHRUNG UND AUFBAU https://custom-build-robots.com/raspberry-pi-elektronik/16-kanal-pca9685-servo-kontroller-i2c-schnittstelle-einfuehrung-aufbau/8040 16 channels From Adafruit Connect to Raspberry via I2C 3.3V supply For motor control together with L298N H-bridge Table 10.1: Wiring PCA9685 Raspberry Pi Raspberry_Pi Servo_Driver Pin 1 (3,3 V) VCC Pin 6 (GND) GND GPIO 2 (SDA) SDA GPIO 3 (SCL) SCL 10.3.2.1.2 Adafruit 16 Channel Servo Driver with Raspberry Pi https://learn.adafruit.com/adafruit-16-channel-servo-driver-with-raspberry-pi/overview Good explanation with link to github code and wiring example 10.3.2.2 Wii sensor Could use wii sensor instead of camera 10.4 Simulators 10.4.1 Donkey Simulator http://docs.donkeycar.com/guide/simulator/ Experiment with training a donkey car to drive in simulation. This simulator is built on the the Unity game platform, uses their internal physics and graphics, and connects to a donkey Python process to use our trained model to control the simulated Donkey. 10.5 Software 10.5.1 Udacitiy open source SDC https://github.com/udacity/self-driving-car At Udacity, we believe in democratizing education. How can we provide opportunity to everyone on the planet? We also believe in teaching really amazing and useful subject matter. When we decided to build the Self-Driving Car Nanodegree program, to teach the world to build autonomous vehicles, we instantly knew we had to tackle our own self-driving car too. Together with Google Self-Driving Car founder and Udacity President Sebastian Thrun, we formed our core Self-Driving Car Team. One of the first decisions we made? Open source code, written by hundreds of students from across the globe! 10.5.2 connect raspberry with xbox controller good tutorial with example code https://tutorials-raspberrypi.de/raspberry-pi-xbox-360-controller-steuern/ you find a code example import RPi.GPIO as GPIO import math import xbox GPIO_LED_GREEN = 23 GPIO_LED_RED = 22 GPIO_LED_YELLOW = 27 GPIO_LED_BLUE = 17 GPIO_SERVO_PIN = 25 GPIO.setmode(GPIO.BCM) GPIO.setwarnings(False) GPIO.setup(GPIO_LED_GREEN, GPIO.OUT) GPIO.setup(GPIO_LED_RED, GPIO.OUT) GPIO.setup(GPIO_LED_YELLOW, GPIO.OUT) GPIO.setup(GPIO_LED_BLUE, GPIO.OUT) GPIO.setup(GPIO_SERVO_PIN, GPIO.OUT) def updateServo(pwm, angle): duty = float(angle) / 10.0 + 2.5 pwm.ChangeDutyCycle(duty) def angleFromCoords(x,y): angle = 0.0 if x==0.0 and y==0.0: angle = 90.0 elif x&gt;=0.0 and y&gt;=0.0: # first quadrant angle = math.degrees(math.atan(y/x)) if x!=0.0 else 90.0 elif x&lt;0.0 and y&gt;=0.0: # second quadrant angle = math.degrees(math.atan(y/x)) angle += 180.0 elif x&lt;0.0 and y&lt;0.0: # third quadrant angle = math.degrees(math.atan(y/x)) angle += 180.0 elif x&gt;=0.0 and y&lt;0.0: # third quadrant angle = math.degrees(math.atan(y/x)) if x!=0.0 else -90.0 angle += 360.0 return angle if __name__ == &#39;__main__&#39;: joy = xbox.Joystick() pwm = GPIO.PWM(GPIO_SERVO_PIN, 100) pwm.start(5) while not joy.Back(): # LEDs led_state_green = GPIO.HIGH if joy.A() else GPIO.LOW led_state_red = GPIO.HIGH if joy.B() else GPIO.LOW led_state_yellow = GPIO.HIGH if joy.Y() else GPIO.LOW led_state_blue = GPIO.HIGH if joy.X() else GPIO.LOW GPIO.output(GPIO_LED_GREEN, led_state_green) GPIO.output(GPIO_LED_RED, led_state_red) GPIO.output(GPIO_LED_YELLOW, led_state_yellow) GPIO.output(GPIO_LED_BLUE, led_state_blue) # Servo x, y = joy.leftStick() angle = angleFromCoords(x,y) if angle &gt; 180 and angle &lt; 270: angle = 180 elif angle &gt;= 270: angle = 0 updateServo(pwm, angle) joy.close() pwm.stop() 10.5.3 RASPBERRY PI RC CONTROL http://www.instructables.com/id/Raspberry-Pi-RC-Control/ 10.5.4 Python class to support xbox 360 controller under Linux on RaspberryPi Python class to support reading xbox 360 wired and wireless controller input under Linux. Makes it easy to get real-time input from controller buttons, analog sticks and triggers. Built and tested on RaspberryPi running Raspbian. code example import xbox joy = xbox.Joystick() #Initialize joystick if joy.A(): #Test state of the A button (1=pressed, 0=not pressed) print &#39;A button pressed&#39; x_axis = joy.leftX() #X-axis of the left stick (values -1.0 to 1.0) (x,y) = joy.leftStick() #Returns tuple containing left X and Y axes (values -1.0 to 1.0) trigger = joy.rightTrigger() #Right trigger position (values 0 to 1.0) joy.close() #Cleanup before exit https://github.com/FRC4564/Xbox 10.5.5 xbox driver for mac https://github.com/360Controller/360Controller/releases 10.5.6 Konzept für softwareschichten Die Softwareschichten sollen es ermöglichen das teile der software zwischen den bastlern ausgetauscht werden können bzw. unabhängig voneinander entwickelt werden können Arduino comm schicht actor schicht input geschwindigkeit cm/s resolution TBD lenkwinkel grad resolution 0.1 degree sensor schicht ultraschall Raspi comm schicht actor schicht arduino USB seriell oder I2C auf PCA wlan/USB für den controller TCP/IP wlan für streaming des videos SAMBA zum auslesen des USB sticks sensor schicht kamera neuronales netz/ oder läuft auf PC USB stick für trainingsdatenaufzeichnung output geschwindigkeit cm/s resolution TBD lenkwinkel grad resolution 0.1 degree input über WLAN vom Laptop geschwindigkeit cm/s resolution TBD lenkwinkel grad resolution 0.1 degree Laptop comm schicht controller USB stick auslesen verbindung zum Raspberry data pre-processing sensor data camera data downsampling color scheme neuronales netz training output geschwindigkeit cm/s resolution TBD lenkwinkel grad resolution 0.1 degree TBC "],
["experiences.html", "Chapter 11 Experiences gathered 11.1 Install donkey car on Mac 11.2 Donkey car simulator", " Chapter 11 Experiences gathered Here I will write down how the adventure goes 11.1 Install donkey car on Mac http://docs.donkeycar.com/guide/install_software/#install-donkeycar-on-mac Installation went without problem. Note: After closing the Terminal, when you open it again, you will need to type source activate donkey to re-enable the mappings to donkey specific Python libraries 11.2 Donkey car simulator http://docs.donkeycar.com/guide/simulator/ Typical Use Start simulator Double check that log dir exists and is empty Start scene of your choice Hit Auto Drive w Rec button Vary the Max Speed, Prop, and Diff sliders to obtain a variety of driving styles Wait 10-15 minutes until you have recorded 10K+ frames of data. Hit the Stop button Hit the Exit button Move the log dir to the ~/d2/data/ dir where you normally put tub data. - This will create a ~/d2/data/log path. Train as usual. "],
["Theory.html", "Chapter 12 Theory 12.1 Wiring of PCA9685 with L298N H-bridge 12.2 sensor fusion of LIDAR and camera data", " Chapter 12 Theory This section is a collection of theory and concepts 12.1 Wiring of PCA9685 with L298N H-bridge Since the raspberry pi PWM signal is not very stable the I2C interface is used to connect to to the servo driver PCA9685. If a higher voltage is needed the motor driver L298N H-bridge can be utilized. Each H-Bridge requires two additional signals: turn motor CW turn motor CCW no signal =&gt; motor brakes https://www.npmjs.com/package/node-red-contrib-pca9685 12.2 sensor fusion of LIDAR and camera data Paper by Varuna De Silva, Jamie Roche, and Ahmet Kondoz, Senior Member, IEEE Fusion of LiDAR and Camera Sensor Data for Environment Sensing in Driverless Vehicles This paper addresses the problem of fusing the outputs of a LiDAR scanner and a wide-angle monocular image sensor. The first part of the proposed framework spatially aligns the two sensor data streams with a geometric model. The resolutions of the two sensors are quite different, with the image sensor having a much denser spatial resolution. https://arxiv.org/pdf/1710.06230.pdf "],
["UsefulLinks.html", "Chapter 13 Useful links 13.1 Donkey car 13.2 Parts to build a RoboCar 13.3 Little setup helpers 13.4 Meetup in Stuttgart area 13.5 RoboCar projects 13.6 Write ups on Donkeycar projects 13.7 Other stuff", " Chapter 13 Useful links Links I found useful at some point 13.1 Donkey car Here are links to the donkey car project A great buying guide for RC-cars Calibrate your car Drive your car Train an autopilot with Keras http://docs.donkeycar.com/guide/simulator/ Controller Parts donkey Car FAQ Donkey car CAD F360 file 13.1.1 Videos for donkey car Tawn Kramer on how to add parts (like a camera) to the vehicle. Great step by step intro to the Vehicle class of the framework. - Donkeycar Parts Overview Pt 1/2 - Donkeycar Parts Overview Pt 2/2 Another video by Tawn Kramer gives tips on how to drive during training Driving Tips to Train your Autonomous End-to-End NN Driver This is the official Donkey Car Assembly video. It includes using options like the Sombrero board and The NVIDIA Jetson Nano Donkey Car Assembly Video 13.2 Parts to build a RoboCar Links to hardware Reely Dart 2.0 Brushed PWM SERVO HAT Adafruit 16-Channel PWM/Servo HAT &amp; Bonnet for Raspberry Pi16 channels of servo-bust Adafruit 16 Channel Servo Driver with Raspberry Pi Raspberry Pi Raspbery Pi camera DC/DC converter for raspi WiiU controller Tamiya Stecker Power wires Template for 3D printing a plate for raspi and DC/DC converter 13.3 Little setup helpers Here are links to instructions on how to get things done SETTING WIFI UP VIA THE COMMAND LINE Install XBox driver on mac Python Xbox Controller Mac find IP adress in shack Self-driving car in a simulator with a tiny neural network by xslittlegrass Github of Detroit-Autonomous-Vehicle-Group HOW TO TETHER RASPBERRY PI TO ANDROID DEVICE HOTSPOT File Sharing From a Mac to Your Raspberry Pi SFTP on Raspbery 13.4 Meetup in Stuttgart area Esslinger Makerspace Projekt: Autonomen RoboCar bauen 13.5 RoboCar projects DonkeyCar DIY RoboCars website Sunfounder RPI car AWS DeepRacer 13.6 Write ups on Donkeycar projects Compact Donkey Car by Oliver Wannenwetsch Learning to Drive Smoothly in Minutes Reinforcement Learning on a Small Racing Car HOW TO SET UP A ROBOCAR PLATFORM WITH A REMOTE CONTROL UNIT by itemis 13.7 Other stuff ESC explained LED on The Adafruit 16-Channel 12-bit PWM/Servo HAT Mount a Raspberry Pi SD card on a Mac (read-only) with osxfuse and ext4fuse Dimming LEDs with PWM/Servo HAT without the need of an restistor in chapter “Python Wiring” cross reference headings in bookdown {#AssignStickToAction} 5.4 "]
]
